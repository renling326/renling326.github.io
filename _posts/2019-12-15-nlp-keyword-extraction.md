---
layout: post
title:  "NLP-关键词抽取的几种方法" 
date:   2019-07-26 15:40:56
categories: jekyll update
tags: [自然语言处理]
---

##TextRank

PageRank
1. 链接数量
2. 连接质量
一般用户自动生成摘要
分析文章中的句子，提取关键语句。计算句子间的相似度

##TF-IDF
TF-IDF = TF*IDF
TF：词频，IDF：逆文档频率
tf=(word在文档中出现的次数)/(word总数)
idf=log((文档总数/(1+word出现的文档数)))
优点：简单
缺点：不够准确，单纯依靠出现的频率

##主题模型
文档和文章没有直接联系，他们应当还有一个维度将他们联系起来，这个维度就是主题。
每个文档都会对应一个或多个主题，而每个主题都会有对应的次分布，通过主题就可以得到每个文档的词分布。
P（word | doc）=P（word | topic）*P（topic | doc）
在一个已知的数据集中，每个文档对应的P（word | doc）都是已知的。

####3.1 LSA（潜在语义分析）/LSI（潜在语义索引）
LSA 的主要步骤：

1. 使用BOW模型将每个文档表示为向量
2. 将所有的文档词向量拼接起来构成词-文档矩阵（M*N）
3. 对词-文档矩阵进行奇异值分解（SVD）操作([m*r][r*r][r*n])
4. 根据SVD结构，将词-文档矩阵映射到一个更低纬度k([m*k][k*k][k*n],0<k<r)的近似结果，每个词和文档都可以表示为K个主题构成的空间中的一个点。通过计算每个词和文档的相似度，可以得到每个文档对每个词的相似度，取相似度最高的一个词作为文档的关键词。

#####LSA优点：
可以映射到地位的空间，并在有限利用文本语义信息的同事，大大降低计算的代价，提高分析的质量。
#####LSA缺点：
1. SVD的计算复杂度非常高，特征空间的维度较大，计算效率十分低下
2. LSAd饿到的分部信息是基于已有的数据集，当一个新的文档进入已有的特征空间时，需要对整个空间进行重新计算，已得到新加入文档后对应的分部信息。
3. LSA对词的频率分布不敏感，物理解释性薄弱的问题

#####3.2LDA
该算法的理论基础是贝叶斯理论
LDA算法假设文档中的主题的先验分布和主题中的词的先验分布都服从狄利克雷分布。
结合吉布斯采样的LDA模型训练过程：
	
1. 随机初始化，对语料中的每篇文档中的每个词W，随机的赋予一个topic编号Z。
2. 重新扫描语料库，对每个词w按照吉布斯采样公式重新采样它的topic，在语料中进行更新。
3. 重复以上语料库的重新采样过程直到吉布斯采样收敛
4. 统计语料库的topic-word共现频率矩阵，该矩阵就是LDA的模型

在我们得到主题对词的分布后，进而得到词对主题的分布。接下来就可以通过分部信息，计算文档与词的相似性，鸡儿得到文档最相似的词列表，最后就可以得到文档的关键词

